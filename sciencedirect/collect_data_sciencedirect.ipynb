{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup \n",
    "from selenium import webdriver\n",
    "from urllib.request import Request, urlopen\n",
    "import pandas as pd\n",
    "import json\n",
    "import sys\n",
    "import time\n",
    "from math import*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PARTIE 1 : COLLECTE DE DONNEES SUR SCIENCEDIRECT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comment utiliser le script ? \n",
    "\n",
    "### 1) Faire une requête sur ScienceDirect avec votre clé de recherche\n",
    "\n",
    "### 2) Modifier le nombre de résultat par page par 100\n",
    "![Result page](images/results_per_page.png)\n",
    "\n",
    "### 3) Copier url de l'api de la première page de recherche dans first_url \n",
    "\n",
    "Pour accéder à l'api : \n",
    "- Effectuer un clique droit sur la page et sélectionner 'Inspect'\n",
    "- Aller dans le menu ouvert à droite et cliquer sur l'onglet 'Network' \n",
    "- Rafraichisser la page (F5)\n",
    "- Effectuer un double clique dans Name sur 'api?talk=...' pour visualiser l'api et copier le lien dans la variable first_url ou copier le lien request url\n",
    "\n",
    "![tutorial api](images/api_sciencedirect.png)\n",
    "\n",
    "### 4) Lancer le script de démarrage de collect\n",
    "Une fenêtre va s'ouvrir avec sélénium, il se peut que l'opération redémarre plusieurs fois.\n",
    "\n",
    "Le nombre d'article et de page s'affichent au \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrer ici api du lien de la première page de recherche \n",
    "first_url = 'https://www.sciencedirect.com/search/api?tak=%28%22multi-label%22%20OR%20%22multilabel%22%29%20AND%20%28%22text%22%20OR%20%E2%80%9Cdocument%E2%80%9D%20OR%20%E2%80%9Cnatural%20language%20process%E2%80%9D%29%20AND%20%28%22class%22%20OR%20%22categorization%22%20OR%20%22tag%22%29&show=100&t=082608fbf78dbf3f72407fceaf94becf53c742c8b4a638c563345288bcadb775&hostname=www.sciencedirect.com'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cette fonctionne transforme la clé de recherche en liste de liens de chaque page de résultat\n",
    "def transform_key_into_api_url(key : str) -> str :\n",
    "    key = key.replace(' ','%20')\n",
    "    key = key.replace('(','%28')\n",
    "    key = key.replace(')','%29')\n",
    "    key = key.replace('\"','%22')\n",
    "    url = 'https://www.sciencedirect.com/search/api?qs=' + key + '&show=100'\n",
    "    return url \n",
    "\n",
    "#Récupère le code JSON de l'API de ScienceDirect et le transforme un dictionnaraire exploitable\n",
    "def API_sciencedirect(url : str) -> dict :\n",
    "    try : \n",
    "        navigator = \"firefox\" # Changer de navigateur ici\n",
    "        if navigator == \"firefox\" : browser = webdriver.Firefox()\n",
    "        elif navigator == \"chrome\": browser = webdriver.Chrome()\n",
    "        elif navigator == \"ie\": browser = webdriver.Ie()\n",
    "        else : raise Exception(\"unable to interpret the navigator\")\n",
    "        browser.get(url)\n",
    "        text = browser.page_source\n",
    "        text = text.replace('<html platform=\"linux\" class=\"theme-light\" dir=\"ltr\"><head><meta http-equiv=\"Content-Security-Policy\" content=\"default-src \\'none\\' ; script-src resource:; \"><link rel=\"stylesheet\" type=\"text/css\" href=\"chrome://devtools-jsonview-styles/content/main.css\"><script type=\"text/javascript\" charset=\"utf-8\" async=\"\" data-requirecontext=\"_\" data-requiremodule=\"viewer-config\" src=\"resource://devtools-client-jsonview/viewer-config.js\"></script></head><body><div id=\"content\"><div id=\"json\">', '')\n",
    "        text = text.replace('</div></div><script src=\"resource://devtools-client-jsonview/lib/require.js\" data-main=\"resource://devtools-client-jsonview/viewer-config.js\"></script></body></html>','')\n",
    "        text = json.loads(text) \n",
    "        browser.close()\n",
    "        return text  \n",
    "    except Exception as e :\n",
    "        print('erreur de chargement de la page... opération recommencée') \n",
    "        time.sleep(10)\n",
    "        browser.close()\n",
    "        API_sciencedirect(url)\n",
    "        \n",
    "# Compte et retourne le nombre de page de résultat de recherche \n",
    "def number_page_counter(url : str) -> int :\n",
    "    try : \n",
    "        source_code = API_sciencedirect(url)\n",
    "        number_article = source_code['resultsFound']\n",
    "        number_page = ceil(int(number_article)/100)\n",
    "        print(\"la recherche contient le nombre suivant d'article :\", number_article)\n",
    "        print(\"reparti sur le nombre suivant de page :\", number_page)\n",
    "        return number_page\n",
    "    except Exception as e :\n",
    "        print(\"nombre de page non récupéré\")\n",
    "        time.sleep(10)\n",
    "        number_page_counter(url)\n",
    "        \n",
    "# Collecte les liens des pages de recherche \n",
    "def find_url_research(first_url : str, number_page : int) -> list :     \n",
    "    url_list=[first_url]\n",
    "    for i in range(1,number_page,1):\n",
    "        i = str(i) # i converti en format string \n",
    "        new_url = first_url + '&offset=' + i + '00&t=bff055c9ae3ffce0274c65a3be0bb20e7fe92ad81e7e672aa07499cfeea8b00b'\n",
    "        url_list.append(new_url)\n",
    "    return url_list\n",
    "\n",
    "# collecte les liens des articles\n",
    "def find_url_articles(url_list : list) -> list :\n",
    "    article_list_url = []\n",
    "    number_page = 1\n",
    "    try :   \n",
    "        for url in url_list : \n",
    "            print('collecte des liens des articles sur la page', number_page)\n",
    "\n",
    "            while True : \n",
    "                source_code = API_sciencedirect(url)\n",
    "                if source_code is not None :\n",
    "                    break\n",
    "            for i in range(100) :\n",
    "                i = int(i)\n",
    "                article_url = str(source_code['searchResults'][i]['pdf']['getAccessLink'])\n",
    "                article_list_url.append(article_url)\n",
    "            number_page = number_page + 1\n",
    "        return article_list_url\n",
    "    except Exception as e :\n",
    "        return article_list_url\n",
    "    \n",
    "# Création d'une dataframe à partir des listes de titre, date, mots clé pour chaque article\n",
    "def create_dataframe(title_list : list ,date_list : list, keywords_list : list):\n",
    "       # crée un dictionnaire à partir des listes\n",
    "        dict = {'Title': title_list, 'Date': date_list , 'Author Keywords': keywords_list}\n",
    "        #utilisation du dictionnaraire pour créer une dataframe\n",
    "        df = pd.DataFrame(dict)\n",
    "        df.to_csv('data/dataSD.csv')  \n",
    "        print(\"Collecte de données terminées\")\n",
    "\n",
    "def collect_data(article_list_url : list) : \n",
    "    title_list=[]\n",
    "    keywords_list=[]\n",
    "    date_list=[]\n",
    "    i = 1\n",
    "    for article in article_list_url :\n",
    "        print(\"chargement des données de l'article\", i) \n",
    "        url='https://www.sciencedirect.com' + article\n",
    "        req = Request(url,headers={'User-Agent': 'Chrome/71.0.3578.98'})\n",
    "        webpage = urlopen(req).read()\n",
    "        soup = BeautifulSoup(webpage,'html.parser')\n",
    "\n",
    "        # store keywords in a list \n",
    "        try : \n",
    "            keywords=soup.find(\"div\",{'class':\"Keywords u-font-serif\"}).getText(',')\n",
    "            keywords=keywords.replace('Keywords,','')\n",
    "            keywords_list.append(keywords)\n",
    "        except Exception as e : \n",
    "            keywords=0\n",
    "            keywords_list.append(keywords)\n",
    "            print(\"Pas de mot clé pour l'article\", i)\n",
    "        # store title in a list \n",
    "        title = soup.find(\"title\").getText()\n",
    "        title = title.replace('- ScienceDirect', '')\n",
    "        title_list.append(title)\n",
    "        # store date in a list \n",
    "        date = soup.find(\"meta\",  {\"name\":\"citation_publication_date\"})\n",
    "        date_list.append(date[\"content\"])    \n",
    "        i=i+1\n",
    "    create_dataframe(title_list ,date_list, keywords_list)\n",
    "\n",
    "def start_collect_data(first_url):\n",
    "    while True :\n",
    "        number_page = number_page_counter(first_url) # récupère le nombre de page de recherche\n",
    "        if number_page is not None :\n",
    "            break\n",
    "    url_list = find_url_research(first_url, number_page) # récupère tout les url des pages de recherche\n",
    "    article_list_url = find_url_articles(url_list) # créer une liste de tout les articles\n",
    "    collect_data(article_list_url) # collecte les titres, dates et mots clés des articles et les stocke dans un csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "erreur de chargement de la page... opération recommencée\n",
      "erreur de chargement de la page... opération recommencée\n",
      "erreur de chargement de la page... opération recommencée\n",
      "nombre de page non récupéré\n",
      "erreur de chargement de la page... opération recommencée\n",
      "nombre de page non récupéré\n",
      "la recherche contient le nombre suivant d'article : 71\n",
      "reparti sur le nombre suivant de page : 1\n",
      "la recherche contient le nombre suivant d'article : 71\n",
      "reparti sur le nombre suivant de page : 1\n",
      "collecte des liens des articles sur la page 1\n",
      "erreur de chargement de la page... opération recommencée\n",
      "erreur de chargement de la page... opération recommencée\n",
      "erreur de chargement de la page... opération recommencée\n",
      "chargement des données de l'article 1\n",
      "Pas de mot clé pour l'article 1\n",
      "chargement des données de l'article 2\n",
      "chargement des données de l'article 3\n",
      "chargement des données de l'article 4\n",
      "chargement des données de l'article 5\n",
      "chargement des données de l'article 6\n",
      "chargement des données de l'article 7\n",
      "chargement des données de l'article 8\n",
      "chargement des données de l'article 9\n",
      "chargement des données de l'article 10\n",
      "chargement des données de l'article 11\n",
      "Pas de mot clé pour l'article 11\n",
      "chargement des données de l'article 12\n",
      "chargement des données de l'article 13\n",
      "chargement des données de l'article 14\n",
      "chargement des données de l'article 15\n",
      "chargement des données de l'article 16\n",
      "chargement des données de l'article 17\n",
      "chargement des données de l'article 18\n",
      "chargement des données de l'article 19\n",
      "chargement des données de l'article 20\n",
      "chargement des données de l'article 21\n",
      "chargement des données de l'article 22\n",
      "chargement des données de l'article 23\n",
      "chargement des données de l'article 24\n",
      "chargement des données de l'article 25\n",
      "chargement des données de l'article 26\n",
      "chargement des données de l'article 27\n",
      "chargement des données de l'article 28\n",
      "chargement des données de l'article 29\n",
      "chargement des données de l'article 30\n",
      "chargement des données de l'article 31\n",
      "chargement des données de l'article 32\n",
      "chargement des données de l'article 33\n",
      "chargement des données de l'article 34\n",
      "chargement des données de l'article 35\n",
      "chargement des données de l'article 36\n",
      "chargement des données de l'article 37\n",
      "chargement des données de l'article 38\n",
      "chargement des données de l'article 39\n",
      "chargement des données de l'article 40\n",
      "chargement des données de l'article 41\n",
      "chargement des données de l'article 42\n",
      "chargement des données de l'article 43\n",
      "chargement des données de l'article 44\n",
      "chargement des données de l'article 45\n",
      "chargement des données de l'article 46\n",
      "chargement des données de l'article 47\n",
      "chargement des données de l'article 48\n",
      "chargement des données de l'article 49\n",
      "chargement des données de l'article 50\n",
      "chargement des données de l'article 51\n",
      "chargement des données de l'article 52\n",
      "chargement des données de l'article 53\n",
      "chargement des données de l'article 54\n",
      "chargement des données de l'article 55\n",
      "chargement des données de l'article 56\n",
      "chargement des données de l'article 57\n",
      "chargement des données de l'article 58\n",
      "chargement des données de l'article 59\n",
      "chargement des données de l'article 60\n",
      "chargement des données de l'article 61\n",
      "chargement des données de l'article 62\n",
      "chargement des données de l'article 63\n",
      "chargement des données de l'article 64\n",
      "chargement des données de l'article 65\n",
      "chargement des données de l'article 66\n",
      "chargement des données de l'article 67\n",
      "chargement des données de l'article 68\n",
      "Pas de mot clé pour l'article 68\n",
      "chargement des données de l'article 69\n",
      "chargement des données de l'article 70\n",
      "Pas de mot clé pour l'article 70\n",
      "chargement des données de l'article 71\n",
      "Collecte de données terminées\n"
     ]
    }
   ],
   "source": [
    "start_collect_data(first_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PARTIE 2 : GENERATION D'UN CSV COMPTANT LES OCCURENCES DE MOTS-CLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# télécharger le csv\n",
    "df = pd.read_csv('data/dataSD.csv')\n",
    "# crée une dataframe avec les \"Author Keywords\"\n",
    "df_words = df[['Author Keywords']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sépare les valeurs séparées par \",\" en colonnes \n",
    "df_words = df_words['Author Keywords'].str.split(\",\", expand = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regroupe toutes les colonnes en une seule colonne\n",
    "df_words = df_words.stack().reset_index()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# supprime les colonnes inutilisées \n",
    "df_words.drop(columns=['level_0'], inplace=True)\n",
    "df_words.drop(columns=['level_1'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# supprime les lignes sans valeur\n",
    "df_words = df_words[df_words!='0']\n",
    "df_words = df_words.dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformer toutes les lettres en minuscule\n",
    "df_words = df_words.applymap(lambda s:s.lower() if type(s) == str else s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# supprimer les espaces à gauche et à droite des mots \n",
    "df_words[0] = (df_words[0]).str.lstrip()\n",
    "df_words[0] = (df_words[0]).str.rstrip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compte le nombre d'occurence des keywords\n",
    "df_words = df_words[0].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# télécharge le csv avec le nombre d'occurence par mot\n",
    "df_words.to_csv('key_words.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
