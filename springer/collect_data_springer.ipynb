{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup \n",
    "from urllib.request import Request, urlopen\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PARTIE 1 : COLLECTE DE DONNEES SUR SPRINGER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lien de la première page de recherche \n",
    "first_url = 'https://rd.springer.com/search?query=multi-label*+AND+OR+AND+multilabel*+AND+AND+AND+text*+AND+OR+AND+document+AND+OR+AND+%E2%80%9Cnatural+AND+language+AND+process*%E2%80%9D+AND+AND+AND+class*+AND+OR+AND+categorization*+AND+OR+AND+tag*&date-facet-mode=between&showAll=true#'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collecte le code source d'une page web d'un url donné en argument \n",
    "def collecte_source_code(url : str) -> str :\n",
    "    try : \n",
    "        req = Request(url,headers={'User-Agent': 'Chrome/71.0.3578.98'})\n",
    "        webpage = urlopen(req).read()\n",
    "        source_code = BeautifulSoup(webpage,'html.parser') \n",
    "        return source_code\n",
    "    except Exception as e :\n",
    "        print('erreur de chargement de la page... opération recommencée') \n",
    "        collecte_source_code(url)\n",
    "\n",
    "# Cette fonction compte le nombre de page de résultat de recherche pour la clé de recherche \n",
    "def collect_link_article_and_conference(first_url : str):\n",
    "    list_url = []\n",
    "    list_url.append(first_url + '&facet-content-type=%22Article%22')\n",
    "    list_url.append(first_url + '&facet-content-type=\"ConferencePaper')\n",
    "    return list_url \n",
    "\n",
    "# Cette fonction compte le nombre de page de résultat de recherche pour la clé de recherche \n",
    "def page_counter(url : str) -> list :\n",
    "    soup = collecte_source_code(url)\n",
    "    try :\n",
    "        number_page  = int(soup.find('span', { \"class\" : \"number-of-pages\"}).getText())\n",
    "    except : \n",
    "        number_page = int(1)\n",
    "    return number_page\n",
    "\n",
    "# Cette fonctionne transforme la clé de recherche en liste de liens de chaque page de résultat\n",
    "def collect_all_url_research(url : str) -> list :\n",
    "    url_list=[url]\n",
    "    number_page = page_counter(url)\n",
    "    print(number_page)\n",
    "    if number_page > 1 :\n",
    "        second_part_url = (url).replace('https://rd.springer.com/search','')\n",
    "        for i in range(2, number_page, 1):\n",
    "            i = str(i)\n",
    "            url_list.append('https://rd.springer.com/search/page/' + i + second_part_url)\n",
    "    return url_list\n",
    "\n",
    "# collecte une liste de titre, date, type de publication, mots clé pour chaque article \n",
    "# et crée une dataframe avec ces données\n",
    "def collect_data(article_list : list, result_type : str) :\n",
    "    i = 1\n",
    "    title_list = []\n",
    "    date_list = []\n",
    "    keywords_list = []\n",
    "    publication_type_list = []\n",
    "    if result_type == 'article':\n",
    "        for article in article_list : \n",
    "            print(\"chargement des données de l'article\", i)\n",
    "            soup = collecte_source_code(article)\n",
    "            try :\n",
    "                title = soup.find('meta', {'name':'dc.title'}).get('content') # title\n",
    "                title_list.append(title)\n",
    "            except Exception as e : \n",
    "                title=0\n",
    "                title_list.append(title)\n",
    "            try : \n",
    "                publication_date = soup.find('meta', {'name':'dc.date'}).get('content') # date\n",
    "                date_list.append(publication_date)\n",
    "            except Exception as e : \n",
    "                publication_date=0\n",
    "                date_list.append(publication_date)\n",
    "            try : \n",
    "                script = soup.find('script', attrs={'type':'text/javascript'}).text\n",
    "                script = script.replace('window.dataLayer = ','')\n",
    "                script = script.replace(';','')\n",
    "                json1_data = json.loads(script)[0] # transformation du script json en dictionnaire \n",
    "                keywords = json1_data['Keywords'] # key words\n",
    "                keywords_list.append(keywords)\n",
    "                print(keywords)\n",
    "            except Exception as e : \n",
    "                keywords=0\n",
    "                keywords_list.append(keywords)\n",
    "            try :     \n",
    "                publication_type = soup.find('meta', {'name':'dc.type'}).get('content') # publication type\n",
    "                publication_type_list.append(publication_type)\n",
    "            except Exception as e : \n",
    "                publication_type = 0 \n",
    "                publication_type_list.append(publication_type)     \n",
    "            i=i+1\n",
    "    else : \n",
    "        for article in article_list : \n",
    "            print(article)\n",
    "            print(\"chargement des données de l'article\", i)\n",
    "            soup = collecte_source_code(article)\n",
    "            try :\n",
    "                title = soup.find('meta', {'name':'citation_title'}).get('content') # title\n",
    "                title_list.append(title)\n",
    "            except Exception as e : \n",
    "                title=0\n",
    "                title_list.append(title)\n",
    "            try : \n",
    "                publication_date = soup.find('meta', {'name':'citation_publication_date'}).get('content') # date\n",
    "                date_list.append(publication_date)\n",
    "            except Exception as e : \n",
    "                publication_date=0\n",
    "                date_list.append(publication_date)\n",
    "            try : \n",
    "                print('keywords',keywords)\n",
    "                keywords = soup.findAll(\"span\",{'class':\"Keyword\"})\n",
    "                keywords = str(keywords)\n",
    "                keywords = keywords.replace('<span class=\"Keyword\">','')\n",
    "                keywords = keywords.replace('</span>','')\n",
    "                keywords = keywords.replace('[','')\n",
    "                keywords = keywords.replace(']','')\n",
    "                print(keywords)\n",
    "                keywords_list.append(keywords)\n",
    "            except Exception as e : \n",
    "                keywords=0\n",
    "                keywords_list.append(keywords)\n",
    "            try :     \n",
    "                publication_type = soup.find('meta', {'name':'dc.type'}).get('content') # publication type\n",
    "                publication_type_list.append(publication_type)\n",
    "            except Exception as e : \n",
    "                publication_type = 0 \n",
    "                publication_type_list.append(publication_type)     \n",
    "            i=i+1\n",
    "    create_dataframe(title_list, date_list, keywords_list, publication_type_list, result_type)\n",
    "    \n",
    "\n",
    "# Création d'une dataframe à partir des listes de titre, date, type de publication, mots clé pour chaque article\n",
    "def create_dataframe(title_list : list, date_list : list, keywords_list : list, publication_type_list : list, result_type : str) :\n",
    "       # crée un dictionnaire à partir des listes\n",
    "        dict = {'Title': title_list, 'Date': date_list , 'Author Keywords': keywords_list, 'Publication type' : publication_type_list}\n",
    "        # utilisation du dictionnaraire pour créer une dataframe\n",
    "        df = pd.DataFrame(dict)\n",
    "        if result_type == 'article':\n",
    "            df.to_csv('data/dataSPRINGER_article.csv')\n",
    "        else : \n",
    "            df.to_csv('data/dataSPRINGER_conference.csv')\n",
    "        return df\n",
    "\n",
    "\n",
    "\n",
    "# collecte les liens des pages de recherche et des articles et en extrait les données \n",
    "def scrapping_springer(url : list, result_type : str) :\n",
    "    #url_list = transform_key_into_url(KEY)\n",
    "    i = 0\n",
    "    url_list = url\n",
    "    article_list = []\n",
    "    url_list_article = []\n",
    "    for url in url_list :\n",
    "        soup = collecte_source_code(url)\n",
    "        alllink = soup.find_all(class_= 'title') # liens des articles\n",
    "        i+=1\n",
    "        print('collecte des liens des articles sur la page', i)\n",
    "        for link in alllink :\n",
    "            article_list.append('https://rd.springer.com/' + link['href']) # tout les liens des articles \n",
    "    print(\"le nombre total d'article est de\", len(article_list))\n",
    "    collect_data(article_list, result_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "collecte des liens des articles sur la page 1\n",
      "le nombre total d'article est de 20\n",
      "chargement des données de l'article 1\n",
      "chargement des données de l'article 2\n",
      "chargement des données de l'article 3\n",
      "Topic models, LDA, Multi-label classification, Document modeling, Text classification, Graphical models, Probabilistic generative models, Dependency-LDA\n",
      "chargement des données de l'article 4\n",
      "\n",
      "chargement des données de l'article 5\n",
      "chargement des données de l'article 6\n",
      "chargement des données de l'article 7\n",
      "Multilabel classification, Instance-based learning, Nearest neighbor classification, Logistic regression, Bayesian inference\n",
      "chargement des données de l'article 8\n",
      "chargement des données de l'article 9\n",
      "chargement des données de l'article 10\n",
      "chargement des données de l'article 11\n",
      "chargement des données de l'article 12\n",
      "Text categorization, Deep belief network, Stochastic gradient descent, CAViaR, Vector space model\n",
      "chargement des données de l'article 13\n",
      "chargement des données de l'article 14\n",
      "LSTM\n",
      "                  \n",
      "                  \n",
      "                  \n",
      "                 (repLSTM, rankLSTM), Multi-label ranking, Document classification, Deep learning, Semantic indexing\n",
      "chargement des données de l'article 15\n",
      "chargement des données de l'article 16\n",
      "E-mail classification, Machine learning, Long short-term memory, Natural language processing\n",
      "chargement des données de l'article 17\n",
      "Text classification, Natural language processing, Knowledge representation, Semantic enrichment, Use case specification\n",
      "chargement des données de l'article 18\n",
      "Multilabel learning, Graph classification, Graph neural networks, Message passing\n",
      "chargement des données de l'article 19\n",
      "chargement des données de l'article 20\n",
      "Text classifier, Experimentation in software engineering, Issue tracker system, Text mining, Label prediction\n",
      "2\n",
      "collecte des liens des articles sur la page 1\n",
      "le nombre total d'article est de 20\n",
      "https://rd.springer.com//chapter/10.1007/978-3-540-24580-3_50\n",
      "chargement des données de l'article 1\n",
      "https://rd.springer.com//chapter/10.1007/978-3-642-12837-0_11\n",
      "chargement des données de l'article 2\n",
      "keywords 0\n",
      "Text Classification , Multilabel Classification , Legal Documents , EUR-Lex Database , Learning by Pairwise Comparison \n",
      "https://rd.springer.com//article/10.1007/s10994-011-5272-5\n",
      "chargement des données de l'article 3\n",
      "keywords Text Classification , Multilabel Classification , Legal Documents , EUR-Lex Database , Learning by Pairwise Comparison \n",
      "\n",
      "https://rd.springer.com//article/10.1007/s11548-015-1213-2\n",
      "chargement des données de l'article 4\n",
      "keywords \n",
      "\n",
      "https://rd.springer.com//chapter/10.1007/978-3-319-12511-4_11\n",
      "chargement des données de l'article 5\n",
      "keywords \n",
      "hierarchical classification , patent classification , IPC , WIPO , patent content , text mining \n",
      "https://rd.springer.com//chapter/10.1007/978-3-319-14331-6_26\n",
      "chargement des données de l'article 6\n",
      "keywords hierarchical classification , patent classification , IPC , WIPO , patent content , text mining \n",
      "Semantic Similarity , Deep Learning , Evaluation Metrics , Cosine Similarity , Deep Learning Method \n",
      "https://rd.springer.com//article/10.1007/s10994-009-5127-5\n",
      "chargement des données de l'article 7\n",
      "keywords Semantic Similarity , Deep Learning , Evaluation Metrics , Cosine Similarity , Deep Learning Method \n",
      "\n",
      "https://rd.springer.com//chapter/10.1007/978-981-15-9433-5_23\n",
      "chargement des données de l'article 8\n",
      "keywords \n",
      "Multilabel classification , Sentiment analysis , Oversampling \n",
      "https://rd.springer.com//chapter/10.1007/978-3-642-20267-4_7\n",
      "chargement des données de l'article 9\n",
      "keywords Multilabel classification , Sentiment analysis , Oversampling \n",
      "Multi-label text classification , text modelling , problem transformation \n",
      "https://rd.springer.com//chapter/10.1007/978-3-540-30134-9_67\n",
      "chargement des données de l'article 10\n",
      "keywords Multi-label text classification , text modelling , problem transformation \n",
      "Test Document , Temporal Sequence , Text Categorization , Vector Space Model , Training Document \n",
      "https://rd.springer.com//chapter/10.1007/978-3-319-08326-1_34\n",
      "chargement des données de l'article 11\n",
      "keywords Test Document , Temporal Sequence , Text Categorization , Vector Space Model , Training Document \n",
      "Target Category , Target Label , Document Categorization , Standard Precision , Category Graph \n",
      "https://rd.springer.com//article/10.1007/s12065-020-00449-x\n",
      "chargement des données de l'article 12\n",
      "keywords Target Category , Target Label , Document Categorization , Standard Precision , Category Graph \n",
      "\n",
      "https://rd.springer.com//chapter/10.1007/978-3-030-69717-4_41\n",
      "chargement des données de l'article 13\n",
      "keywords \n",
      "Arabic text classification , Multilabel tagging , Natural Language Processing , Shallow Learning , Deep learning \n",
      "https://rd.springer.com//article/10.1007/s11063-017-9636-0\n",
      "chargement des données de l'article 14\n",
      "keywords Arabic text classification , Multilabel tagging , Natural Language Processing , Shallow Learning , Deep learning \n",
      "\n",
      "https://rd.springer.com//chapter/10.1007/978-3-319-93935-3_6\n",
      "chargement des données de l'article 15\n",
      "keywords \n",
      "\n",
      "https://rd.springer.com//article/10.1007/s00521-020-05058-4\n",
      "chargement des données de l'article 16\n",
      "keywords \n",
      "\n",
      "https://rd.springer.com//article/10.1007/s10579-017-9406-7\n",
      "chargement des données de l'article 17\n",
      "keywords \n",
      "\n",
      "https://rd.springer.com//article/10.1007/s10994-019-05782-6\n",
      "chargement des données de l'article 18\n",
      "keywords \n",
      "\n",
      "https://rd.springer.com//chapter/10.1007/978-3-642-17432-2_11\n",
      "chargement des données de l'article 19\n",
      "keywords \n",
      "Average Method , Information Gain , Average Precision , Text Categorization , Maximum Method \n",
      "https://rd.springer.com//article/10.1007/s13748-019-00182-2\n",
      "chargement des données de l'article 20\n",
      "keywords Average Method , Information Gain , Average Precision , Text Categorization , Maximum Method \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# lance le scraping sur springer\n",
    "first_page_url = collect_link_article_and_conference(first_url)\n",
    "#scrapping_springer(first_page_url[0])\n",
    "url_research = collect_all_url_research(first_page_url[0])\n",
    "df_article = scrapping_springer(url_research,'article')\n",
    "url_research = collect_all_url_research(first_page_url[1])\n",
    "df_conference = scrapping_springer(url_research, 'conference')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PARTIE 2 : GENERATION D'UN CSV COMPTANT LES OCCURENCES DE MOTS-CLE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# télécharger le csv\n",
    "df = pd.read_csv('dataSPRINGER.csv')\n",
    "# crée une dataframe avec les \"Author Keywords\"\n",
    "df_words = df[['Author Keywords']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sépare les valeurs séparées par \",\" en colonnes \n",
    "df_words = df_words['Author Keywords'].str.split(\",\", expand = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regroupe toutes les colonnes en une seule colonne\n",
    "df_words = df_words.stack().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# supprime les colonnes inutilisées \n",
    "df_words.drop(columns=['level_0'], inplace=True)\n",
    "df_words.drop(columns=['level_1'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# supprime les lignes sans valeur\n",
    "df_words = df_words.dropna()\n",
    "df_words = df_words[df_words!=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformer toutes les lettres en minuscule\n",
    "df_words = df_words.applymap(lambda s:s.lower() if type(s) == str else s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# supprimer les espaces à gauche et à droite des mots \n",
    "df_words[0] = (df_words[0]).str.lstrip()\n",
    "df_words[0] = (df_words[0]).str.rstrip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compte le nombre d'occurence des keywords\n",
    "df_words = df_words[0].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# télécharge le csv avec le nombre d'occurence par mot\n",
    "df_words.to_csv('key_words.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
