{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup \n",
    "from urllib.request import Request, urlopen\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PARTIE 1 : COLLECTE DE DONNEES SUR SPRINGER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clé du moteur de recherche Springer \n",
    "first_url = ['https://rd.springer.com/search?date-facet-mode=between&showAll=true&query=multi-label*+AND+OR+AND+multilabel*+AND+AND+AND+text*+AND+OR+AND+document+AND+OR+AND+%E2%80%9Cnatural+AND+language+AND+process*%E2%80%9D+AND+AND+AND+class*+AND+OR+AND+categorization*+AND+OR+AND+tag*&facet-content-type=%22ConferencePaper%22']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collecte le code source d'une page web d'un url donné en argument \n",
    "def collecte_source_code(url : str) -> str :\n",
    "    try : \n",
    "        req = Request(url,headers={'User-Agent': 'Chrome/71.0.3578.98'})\n",
    "        webpage = urlopen(req).read()\n",
    "        source_code = BeautifulSoup(webpage,'html.parser') \n",
    "        return source_code\n",
    "    except Exception as e :\n",
    "        print('erreur de chargement de la page... opération recommencée') \n",
    "        collecte_source_code(url)\n",
    "\n",
    "# Cette fonction compte le nombre de page de résultat de recherche pour la clé de recherche \n",
    "def page_counter(first_url : str) -> int :\n",
    "    soup = collecte_source_code(first_url)\n",
    "    number_page  = soup.find('span', { \"class\" : \"number-of-pages\"}).getText()\n",
    "    return number_page  \n",
    "\n",
    "# Cette fonctionne transforme la clé de recherche en liste de liens de chaque page de résultat\n",
    "def transform_key_into_url(key : str) -> list :    \n",
    "    key = key.replace(' ','+')\n",
    "    key = key.replace('(','%28')\n",
    "    key = key.replace(')','%29')\n",
    "    key = key.replace('\"','%27')\n",
    "    url_list = ['https://rd.springer.com/search?query=' + key + '&facet-discipline=\"Computer+Science\"&facet-content-type=\"Article\"']\n",
    "    number_page = int(page_counter(url_list[0]))\n",
    "    for i in range(2, number_page, 1):\n",
    "        i = str(i)\n",
    "        url_list.append('https://rd.springer.com/search/page/' + i +'?query=%28%27Natural+Language+Processing%27%29+AND+%28%27Multi-label%27%29&facet-discipline=%22Computer+Science%22&facet-content-type=%22Article%22')\n",
    "    return url_list\n",
    "\n",
    "# collecte une liste de titre, date, type de publication, mots clé pour chaque article \n",
    "# et crée une dataframe avec ces données\n",
    "def collect_data(article_list : list) :\n",
    "    i = 1\n",
    "    title_list = []\n",
    "    date_list = []\n",
    "    keywords_list = []\n",
    "    publication_type_list = []\n",
    "    for article in article_list : \n",
    "        print(\"chargement des données de l'article\", i)\n",
    "        soup = collecte_source_code(article)\n",
    "        try :\n",
    "            title = soup.find('meta', {'name':'dc.title'}).get('content') # title\n",
    "            title_list.append(title)\n",
    "        except Exception as e : \n",
    "            title=0\n",
    "            title_list.append(title)\n",
    "        try : \n",
    "            publication_date = soup.find('meta', {'name':'dc.date'}).get('content') # date\n",
    "            date_list.append(publication_date)\n",
    "        except Exception as e : \n",
    "            publication_date=0\n",
    "            date_list.append(publication_date)\n",
    "        try : \n",
    "            script = soup.find('script', attrs={'type':'text/javascript'}).text\n",
    "            script = script.replace('window.dataLayer = ','')\n",
    "            script = script.replace(';','')\n",
    "            json1_data = json.loads(script)[0] # transformation du script json en dictionnaire \n",
    "            keywords = json1_data['Keywords'] # key words\n",
    "            keywords_list.append(keywords)\n",
    "        except Exception as e : \n",
    "            keywords=0\n",
    "            keywords_list.append(keywords)\n",
    "        try :     \n",
    "            publication_type = soup.find('meta', {'name':'dc.type'}).get('content') # publication type\n",
    "            publication_type_list.append(publication_type)\n",
    "        except Exception as e : \n",
    "            publication_type = 0 \n",
    "            publication_type_list.append(publication_type)     \n",
    "        i=i+1\n",
    "    create_dataframe(title_list, date_list, keywords_list, publication_type_list)\n",
    "    \n",
    "def collect_data_chapter(article_list : list) :\n",
    "    i = 1\n",
    "    title_list = []\n",
    "    date_list = []\n",
    "    keywords_list = []\n",
    "    publication_type_list = []\n",
    "    for article in article_list : \n",
    "        print(\"chargement des données de l'article\", i)\n",
    "        soup = collecte_source_code(article)\n",
    "        try :\n",
    "            title = soup.find('meta', {'name':'citation_title'}).get('content') # title\n",
    "            title_list.append(title)\n",
    "        except Exception as e : \n",
    "            title=0\n",
    "            title_list.append(title)\n",
    "        try : \n",
    "            publication_date = soup.find('meta', {'name':'citation_publication_date'}).get('content') # date\n",
    "            date_list.append(publication_date)\n",
    "        except Exception as e : \n",
    "            publication_date=0\n",
    "            date_list.append(publication_date)\n",
    "        try : \n",
    "            keywords = soup.findAll(\"span\",{'class':\"Keyword\"})\n",
    "            keywords = str(keywords)\n",
    "            keywords = keywords.replace('<span class=\"Keyword\">','')\n",
    "            keywords = keywords.replace('</span>','')\n",
    "            keywords = keywords.replace('[','')\n",
    "            keywords = keywords.replace(']','')\n",
    "            keywords_list.append(keywords)\n",
    "        except Exception as e : \n",
    "            keywords=0\n",
    "            keywords_list.append(keywords)\n",
    "        try :     \n",
    "            publication_type = soup.find('meta', {'name':'dc.type'}).get('content') # publication type\n",
    "            publication_type_list.append(publication_type)\n",
    "        except Exception as e : \n",
    "            publication_type = 0 \n",
    "            publication_type_list.append(publication_type)     \n",
    "        i=i+1\n",
    "    create_dataframe(title_list, date_list, keywords_list, publication_type_list)\n",
    "\n",
    "# Création d'une dataframe à partir des listes de titre, date, type de publication, mots clé pour chaque article\n",
    "def create_dataframe(title_list : list, date_list : list, keywords_list : list, publication_type_list : list):\n",
    "       # crée un dictionnaire à partir des listes\n",
    "        dict = {'Title': title_list, 'Date': date_list , 'Author Keywords': keywords_list, 'Publication type' : publication_type_list}\n",
    "        # utilisation du dictionnaraire pour créer une dataframe\n",
    "        df = pd.DataFrame(dict)\n",
    "        df.to_csv('data/dataSPRINGER.csv')\n",
    "\n",
    "# collecte les liens des pages de recherche et des articles et en extrait les données \n",
    "def scrapping_springer(url : list) :\n",
    "    #url_list = transform_key_into_url(KEY)\n",
    "    i = 0\n",
    "    url_list = url\n",
    "    article_list = []\n",
    "    url_list_article = []\n",
    "    for url in url_list :\n",
    "        soup = collecte_source_code(url)\n",
    "        alllink = soup.find_all(class_= 'title') # liens des articles\n",
    "        i+=1\n",
    "        print('collecte des liens des articles sur la page', i)\n",
    "        for link in alllink :\n",
    "            article_list.append('https://rd.springer.com/' + link['href']) # tout les liens des articles \n",
    "    print(\"le nombre total d'article est de\", len(article_list))\n",
    "    collect_data_chapter(article_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collecte des liens des articles sur la page 1\n",
      "le nombre total d'article est de 14\n",
      "chargement des données de l'article 1\n",
      "chargement des données de l'article 2\n",
      "chargement des données de l'article 3\n",
      "chargement des données de l'article 4\n",
      "chargement des données de l'article 5\n",
      "chargement des données de l'article 6\n",
      "chargement des données de l'article 7\n",
      "chargement des données de l'article 8\n",
      "chargement des données de l'article 9\n",
      "chargement des données de l'article 10\n",
      "chargement des données de l'article 11\n",
      "chargement des données de l'article 12\n",
      "chargement des données de l'article 13\n",
      "chargement des données de l'article 14\n"
     ]
    }
   ],
   "source": [
    "# lance le scraping sur springer\n",
    "scrapping_springer(first_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PARTIE 2 : GENERATION D'UN CSV COMPTANT LES OCCURENCES DE MOTS-CLE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# télécharger le csv\n",
    "df = pd.read_csv('dataSPRINGER.csv')\n",
    "# crée une dataframe avec les \"Author Keywords\"\n",
    "df_words = df[['Author Keywords']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sépare les valeurs séparées par \",\" en colonnes \n",
    "df_words = df_words['Author Keywords'].str.split(\",\", expand = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regroupe toutes les colonnes en une seule colonne\n",
    "df_words = df_words.stack().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# supprime les colonnes inutilisées \n",
    "df_words.drop(columns=['level_0'], inplace=True)\n",
    "df_words.drop(columns=['level_1'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# supprime les lignes sans valeur\n",
    "df_words = df_words.dropna()\n",
    "df_words = df_words[df_words!=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformer toutes les lettres en minuscule\n",
    "df_words = df_words.applymap(lambda s:s.lower() if type(s) == str else s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# supprimer les espaces à gauche et à droite des mots \n",
    "df_words[0] = (df_words[0]).str.lstrip()\n",
    "df_words[0] = (df_words[0]).str.rstrip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compte le nombre d'occurence des keywords\n",
    "df_words = df_words[0].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# télécharge le csv avec le nombre d'occurence par mot\n",
    "df_words.to_csv('key_words.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
