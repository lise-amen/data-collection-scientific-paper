{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup \n",
    "from urllib.request import Request, urlopen\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PARTIE 1 : COLLECTE DE DONNEES SUR SCIENCEDIRECT\n",
    "\n",
    "## Comment utiliser le script ? \n",
    "\n",
    "### 1) Faire une requête sur Springer avec votre clé de recherche\n",
    "\n",
    "### 2) Copier url de la première page de recherche dans first_url \n",
    "\n",
    "### 3) Lancer le script de démarrage de collecte\n",
    "Lors de l'excécution du script, le nombre de page et d'article s'affichent. \n",
    "\n",
    "Le chargement de données est indiqué pour chaque article. \n",
    "\n",
    "Les données collectées sont : 'article', 'conference paper', 'conference paper and chapter'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lien de la première page de recherche \n",
    "first_url='https://rd.springer.com/search/page/1?date-facet-mode=between&showAll=true&query=multi-label*+AND+OR+AND+multilabel*+AND+AND+AND+text*+AND+OR+AND+document+AND+OR+AND+%E2%80%9Cnatural+AND+language+AND+process*%E2%80%9D+AND+AND+AND+class*+AND+OR+AND+categorization*+AND+OR+AND+tag*'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collecte le code source d'une page web d'un url donné en argument \n",
    "def collecte_source_code(url : str) -> str :\n",
    "    try : \n",
    "        req = Request(url,headers={'User-Agent': 'Chrome/71.0.3578.98'})\n",
    "        webpage = urlopen(req).read()\n",
    "        source_code = BeautifulSoup(webpage,'html.parser') \n",
    "        return source_code\n",
    "    except Exception as e :\n",
    "        print('erreur de chargement de la page... opération recommencée') \n",
    "        collecte_source_code(url)\n",
    "\n",
    "# Cette fonction compte le nombre de page de résultat pour le premier lien de recherche\n",
    "def page_counter(url : str) -> list :\n",
    "    soup = collecte_source_code(url)\n",
    "    try :\n",
    "        number_page  = int(soup.find('span', { \"class\" : \"number-of-pages\"}).getText())\n",
    "    except : \n",
    "        number_page = int(1)\n",
    "    return number_page\n",
    "\n",
    "# collecte une liste de titre, date, type de publication, mots clé pour chaque article \n",
    "# et crée une dataframe avec ces données\n",
    "def collect_data(article_list : list) :\n",
    "    i = 1\n",
    "    title_list = []\n",
    "    date_list = []\n",
    "    keywords_list = []\n",
    "    publication_type_list = []\n",
    "    for article in article_list : \n",
    "        print(\"chargement des données de l'article\", i)\n",
    "        soup = collecte_source_code(article)\n",
    "        try :\n",
    "            title = soup.find('meta', {'name':'dc.title'}).get('content') # title\n",
    "            title_list.append(title)\n",
    "        except : \n",
    "            try :\n",
    "                title = soup.find('meta', {'name':'citation_title'}).get('content') # title\n",
    "                title_list.append(title)\n",
    "            except :\n",
    "                title=0\n",
    "                title_list.append(title)\n",
    "        try : \n",
    "            publication_date = soup.find('meta', {'name':'dc.date'}).get('content') # date\n",
    "            date_list.append(publication_date) \n",
    "        except :\n",
    "            try :\n",
    "                publication_date = soup.find('meta', {'name':'citation_date'}).get('content') # title\n",
    "                date_list.append(publication_date)\n",
    "            except :\n",
    "                publication_date = soup.find('meta', {'name':'citation_publication_date'}).get('content') # title\n",
    "                date_list.append(publication_date)\n",
    "        try : \n",
    "            script = soup.find('script', attrs={'type':'text/javascript'}).text\n",
    "            script = script.replace('window.dataLayer = ','')\n",
    "            script = script.replace(';','')\n",
    "            json1_data = json.loads(script)[0] # transformation du script json en dictionnaire \n",
    "            keywords = json1_data['Keywords'] # key words\n",
    "            keywords_list.append(keywords)\n",
    "        except :\n",
    "            try :\n",
    "                keywords = soup.findAll(\"span\",{'class':\"Keyword\"})\n",
    "                keywords = str(keywords)\n",
    "                keywords = keywords.replace('<span class=\"Keyword\">','')\n",
    "                keywords = keywords.replace('</span>','')\n",
    "                keywords = keywords.replace('[','')\n",
    "                keywords = keywords.replace(']','')\n",
    "                keywords_list.append(keywords)\n",
    "            except : \n",
    "                keywords=0\n",
    "                keywords_list.append(keywords)\n",
    "        try :     \n",
    "            publication_type = soup.find('meta', {'name':'dc.type'}).get('content') # publication type\n",
    "            publication_type_list.append(publication_type)\n",
    "        except : \n",
    "            try : \n",
    "                publication_type = soup.find('meta', {'name':'citation_conference_title'}).get('content') # title\n",
    "                publication_type = 'Conference Paper (Or Conference Paper And Chapter)' \n",
    "                publication_type_list.append(publication_type)  \n",
    "            except : \n",
    "                publication_type = 'Chapter'\n",
    "                publication_type_list.append(publication_type)  \n",
    "        i=i+1\n",
    "    create_dataframe(title_list, date_list, keywords_list, publication_type_list)\n",
    "\n",
    "# Création d'une dataframe à partir des listes de titre, date, type de publication, mots clé pour chaque article\n",
    "def create_dataframe(title_list : list, date_list : list, keywords_list : list, publication_type_list : list) :\n",
    "       # crée un dictionnaire à partir des listes\n",
    "        dict = {'Title': title_list, 'Date': date_list , 'Author Keywords': keywords_list, 'Publication type' : publication_type_list}\n",
    "        # utilisation du dictionnaraire pour créer une dataframe\n",
    "        df = pd.DataFrame(dict)\n",
    "        df = df[df['Publication type'] != 'Chapter']\n",
    "        df.to_csv('data/dataSPRINGER.csv')\n",
    "\n",
    "\n",
    "# collecte les liens des pages de recherche et des articles et en extrait les données \n",
    "def scrapping_springer(url_list : list) :\n",
    "    i = 0\n",
    "    article_list = []\n",
    "    url_list_article = []\n",
    "    for url in url_list :\n",
    "        soup = collecte_source_code(url)\n",
    "        alllink = soup.find_all(class_= 'title') # liens des articles\n",
    "        i+=1\n",
    "        print('collecte des liens des articles sur la page', i)\n",
    "        for link in alllink :\n",
    "            article_list.append('https://rd.springer.com/' + link['href']) # tout les liens des articles           \n",
    "    print(\"le nombre total d'article est de\", len(article_list))\n",
    "    collect_data(article_list)\n",
    "\n",
    "# Cette fonction compte le nombre de page de résultat de recherche pour la clé de recherche \n",
    "def page_counter(url : str) -> int :\n",
    "    soup = collecte_source_code(url)\n",
    "    try :\n",
    "        number_page  = int(soup.find('span', { \"class\" : \"number-of-pages\"}).getText())\n",
    "    except : \n",
    "        number_page = int(1)\n",
    "    return number_page\n",
    "\n",
    "# Cette fonction récupère les liens de chaque page de résultat\n",
    "def collect_all_url_research(url : str) -> list :\n",
    "    url_list = collect_all_url_research(first_url) # récupère les liens de chaque page\n",
    "    url_list=[url]\n",
    "    number_page = page_counter(url)\n",
    "    if number_page > 1 :\n",
    "        second_part_url = (url).replace('https://rd.springer.com/search?','')\n",
    "        second_part_url = second_part_url.replace('*&date-facet-mode=between&showAll=true','')\n",
    "        for i in range(2, number_page + 1, 1):\n",
    "            i = str(i)\n",
    "            new_url = 'https://rd.springer.com/search/page/' + i + '?' + second_part_url\n",
    "            url_list.append(new_url)\n",
    "    return url_list # collecte toute les données "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collecte des liens des articles sur la page 1\n",
      "collecte des liens des articles sur la page 2\n",
      "le nombre total d'article est de 40\n",
      "chargement des données de l'article 1\n",
      "chargement des données de l'article 2\n",
      "chargement des données de l'article 3\n",
      "chargement des données de l'article 4\n",
      "chargement des données de l'article 5\n",
      "chargement des données de l'article 6\n",
      "chargement des données de l'article 7\n",
      "chargement des données de l'article 8\n",
      "chargement des données de l'article 9\n",
      "chargement des données de l'article 10\n",
      "chargement des données de l'article 11\n",
      "chargement des données de l'article 12\n",
      "chargement des données de l'article 13\n",
      "chargement des données de l'article 14\n",
      "chargement des données de l'article 15\n",
      "chargement des données de l'article 16\n",
      "chargement des données de l'article 17\n",
      "chargement des données de l'article 18\n",
      "chargement des données de l'article 19\n",
      "chargement des données de l'article 20\n",
      "chargement des données de l'article 21\n",
      "chargement des données de l'article 22\n",
      "chargement des données de l'article 23\n",
      "chargement des données de l'article 24\n",
      "chargement des données de l'article 25\n",
      "chargement des données de l'article 26\n",
      "chargement des données de l'article 27\n",
      "chargement des données de l'article 28\n",
      "chargement des données de l'article 29\n",
      "chargement des données de l'article 30\n",
      "chargement des données de l'article 31\n",
      "chargement des données de l'article 32\n",
      "chargement des données de l'article 33\n",
      "chargement des données de l'article 34\n",
      "chargement des données de l'article 35\n",
      "chargement des données de l'article 36\n",
      "chargement des données de l'article 37\n",
      "chargement des données de l'article 38\n",
      "chargement des données de l'article 39\n",
      "chargement des données de l'article 40\n"
     ]
    }
   ],
   "source": [
    "scrapping_springer(url_list) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
